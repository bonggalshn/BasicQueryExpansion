{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY FOR DATA LOADING\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# LIBRARY FOR PREPROCESSING\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# LIBRARY FOR RANKED RETRIEVAL\n",
    "import math\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING FUNCTION\n",
    "\n",
    "def loadData(location):\n",
    "    data = ET.parse(location)\n",
    "    return data\n",
    "\n",
    "def docNumber(location):\n",
    "    docNo = []\n",
    "    data = loadData(location)\n",
    "    for node in data.iter(\"DOCNO\"):\n",
    "        docNo.append(node.text)\n",
    "    return docNo\n",
    "\n",
    "def docHeadline(location):\n",
    "    docHead = []\n",
    "    data = loadData(location)\n",
    "    for node in data.iter(\"HEADLINE\"):\n",
    "        docHead.append(node.text)\n",
    "    return docHead\n",
    "    \n",
    "def docText(location):\n",
    "    docText = []\n",
    "    data = loadData(location)\n",
    "    for node in data.iter(\"TEXT\"):\n",
    "        docText.append(node.text)\n",
    "    return docText\n",
    "    \n",
    "def removePunctuation(textList):\n",
    "    for i in range(len(textList)):\n",
    "        for punct in string.punctuation:\n",
    "            textList[i] = textList[i].replace(punct, \" \")\n",
    "        textList[i] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', textList[i], flags=re.MULTILINE)\n",
    "    return textList\n",
    "\n",
    "def caseFolding(textList):\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append(textList[i].lower())\n",
    "    return text\n",
    "\n",
    "def token(sentence):\n",
    "    token = []\n",
    "    for word in CountVectorizer().build_tokenizer()(sentence):\n",
    "        token.append(word)\n",
    "    return token\n",
    "\n",
    "def tokenize(textList):\n",
    "    tokens = []\n",
    "    for i in range(len(textList)):\n",
    "        tokens.append(token(textList[i]))\n",
    "    return tokens\n",
    "\n",
    "def checkStopword(sentence, stop_words):\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    return sentence\n",
    "    \n",
    "def stopwordRemove(textList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append(checkStopword(textList[i], stop_words))\n",
    "    return text\n",
    "\n",
    "def numberRemove(textList):\n",
    "    text = []\n",
    "    for i in range(len(textList)):\n",
    "        text.append([w for w in textList[i] if not any(j.isdigit() for j in w)])\n",
    "    return text\n",
    "\n",
    "def stemming(textList):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = textList\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            text[i][j] = stemmer.stem(text[i][j])\n",
    "    return text\n",
    "\n",
    "def sorting(textList):\n",
    "    for i in range(len(textList)):\n",
    "        textList[i] = sorted(textList[i])\n",
    "    return textList\n",
    "\n",
    "def getAllTerms(textList):\n",
    "    terms = []\n",
    "    for i in range(len(textList)):\n",
    "        for j in range(len(textList[i])):\n",
    "            terms.append(textList[i][j])\n",
    "    return sorted(set(terms))\n",
    "\n",
    "# INDEXING FUNCTION\n",
    "\n",
    "def createIndex(textList, docno):\n",
    "    terms = getAllTerms(textList)\n",
    "    proximity = {}\n",
    "    for term in terms:\n",
    "        position = {}\n",
    "        for n in range(len(textList)):\n",
    "            if(term in textList[n]):\n",
    "                position[docno[n]] = []\n",
    "                for i in range(len(textList[n])):\n",
    "                    if(term == textList[n][i]):\n",
    "                        position[docno[n]].append(i)\n",
    "        proximity[term] = position\n",
    "    return proximity\n",
    "\n",
    "def exportIndex(index, filename):\n",
    "    file = open(filename,'w')\n",
    "    for n in index:\n",
    "        file.write(n+'\\n')\n",
    "        for o in index[n]:\n",
    "            file.write('\\t'+o+': ')\n",
    "            for p in range(len(index[n][o])):\n",
    "                file.write(str(index[n][o][p]))\n",
    "                if(p<len(index[n][o])-1):\n",
    "                    file.write(', ')\n",
    "                else:\n",
    "                    file.write('\\n')\n",
    "    file.close()\n",
    "    return \"Index's file has been successfully created.\"\n",
    "\n",
    "\n",
    "# RANKED RETRIEVAL FUNCTION\n",
    "def queryInIndex(query, index):\n",
    "    result = []\n",
    "    for word in query:\n",
    "        if word in index:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "def df(query, index):\n",
    "    docFreq = {}\n",
    "    for word in query:\n",
    "        if word in index:\n",
    "            docFreq[word] = len(index[word])\n",
    "    return docFreq\n",
    "\n",
    "def idf(df, N):\n",
    "    inv = {}\n",
    "    for word in df:\n",
    "        inv[word] = math.log10(N/df[word])\n",
    "    return inv\n",
    "\n",
    "def tf(query, index):\n",
    "    termFreq = {}\n",
    "    for word in query:\n",
    "        freq = {}\n",
    "        if word in index:\n",
    "            for i in index[word]:\n",
    "                freq[i] = len(index[word][i])\n",
    "        termFreq[word] = freq\n",
    "    return termFreq\n",
    "\n",
    "def tfidf(tf, idf):\n",
    "    w = {}\n",
    "    for word in tf:\n",
    "        wtd = {}\n",
    "        for doc in tf[word]:\n",
    "            wtd[doc] = (1+(math.log10(tf[word][doc])))*idf[word]\n",
    "        w[word] = wtd\n",
    "    return w\n",
    "    \n",
    "def score(TFIDF):\n",
    "    res = {}\n",
    "    for i in TFIDF:\n",
    "        for j in TFIDF[i]:\n",
    "            res[j] = 0\n",
    "    for i in TFIDF:\n",
    "        for j in TFIDF[i]:\n",
    "            res[j] = res[j]+TFIDF[i][j]\n",
    "    sorted_dict = sorted(res, key=res.get, reverse=True)\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Index's file has been successfully created.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "location         = 'collection/trec.sample.xml'\n",
    "documentNumber   = docNumber(location)\n",
    "documentHeadline = docHeadline(location)\n",
    "documentText     = docText(location)\n",
    "documentTotal    = len(documentNumber)\n",
    "text             = []\n",
    "\n",
    "\n",
    "for i in range(documentTotal):\n",
    "    text.append(documentHeadline[i] + documentText[i])\n",
    "\n",
    "# PREPROCESSING\n",
    "text = removePunctuation(text)\n",
    "text = caseFolding(text)\n",
    "text = tokenize(text)\n",
    "text = stopwordRemove(text)\n",
    "text = numberRemove(text)\n",
    "text = stemming(text)\n",
    "\n",
    "\n",
    "# GET ALL TERMS IN COLLECTION\n",
    "\n",
    "terms = getAllTerms(text)\n",
    "\n",
    "# INDEXING\n",
    "\n",
    "# index = createIndex(text,documentNumber, terms)\n",
    "index = createIndex(text,documentNumber)\n",
    "\n",
    "# CREATE INDEX FILE\n",
    "exportIndex(index, 'INDEX.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['abandon abbot company'] \n",
      "\n",
      "\n",
      "RESULTS: \n",
      "\n",
      "\n",
      "FT  14 MAY 91 / International Company News: Temporary rescue for French\n",
      "group\n",
      "Document Number:  31\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / London Stock Exchange: FT-SE 2,500 lost in nervous trading\n",
      "Document Number:  12\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / Leading Article: A compromise path to Emu\n",
      "Document Number:  48\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / UK News (Employment): Harrods to be asked to abandon pay\n",
      "freeze\n",
      "Document Number:  74\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / Uphill fight in the high street / Chart the continuing\n",
      "struggle of Britain's consumer services sector as it fights to survive\n",
      "recession\n",
      "Document Number:  55\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QUERY\n",
    "\n",
    "raw_query = [\"abandon abbot company\"]\n",
    "# raw_query = newQuery\n",
    "\n",
    "query = removePunctuation(raw_query)\n",
    "query = caseFolding(query)\n",
    "query = tokenize(query)\n",
    "query = stopwordRemove(query)\n",
    "query = numberRemove(query)\n",
    "query = stemming(query)\n",
    "query = query[0]\n",
    "\n",
    "# Check Query In Index\n",
    "query = queryInIndex(query, index)\n",
    "\n",
    "# RANKED RETRIEVAL\n",
    "\n",
    "N               = documentTotal\n",
    "tfidf_list      = []\n",
    "\n",
    "docFrequency    = df(query, index)\n",
    "invDocFrequency = idf(docFrequency, N)\n",
    "termFrequency   = tf(query, index)\n",
    "TFIDF           = tfidf(termFrequency, invDocFrequency)\n",
    "sc              = score(TFIDF)\n",
    "\n",
    "relevanceDocNumber = []\n",
    "count = 0\n",
    "print('Query: ', raw_query,'\\n\\n')\n",
    "print('RESULTS: \\n')\n",
    "\n",
    "for i in range(len(sc)):\n",
    "    relevanceDocNumber.append(int(sc[i]))\n",
    "    a = documentNumber.index(sc[i])\n",
    "    print(documentHeadline[a]+ 'Document Number: ',sc[i])\n",
    "#     print('\\nContent:')\n",
    "#     print(documentText[a][0:400], '[read more]>>')\n",
    "    print('-------------------------------------------\\n')\n",
    "    count = count + 1\n",
    "    if(count>=5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPANSION\n",
    "def notRelevance(rel, docsNumber):\n",
    "    notRelevanceNumber = []\n",
    "    for i in docsNumber:\n",
    "        if int(i) not in (rel):\n",
    "            notRelevanceNumber.append(int(i))\n",
    "    return notRelevanceNumber\n",
    "\n",
    "def getIndex(docs, docsNumber):\n",
    "    res = []\n",
    "    for i in docs:\n",
    "        res.append(docsNumber.index(str(i)))\n",
    "    return res\n",
    "\n",
    "def vector(text ,terms):\n",
    "    Vec = []\n",
    "    for i in range(len(terms)):\n",
    "        if(terms[i] in text):\n",
    "            Vec.append(1)\n",
    "        else:\n",
    "            Vec.append(0)\n",
    "    return Vec\n",
    "\n",
    "def expansion(query, relevan, irrelevan, a, b, c):\n",
    "    result = {}\n",
    "    exp    = []\n",
    "    irrel  = irrelevan[0]\n",
    "    rel    = relevan[0]\n",
    "    b      = b / len(relevan)\n",
    "    c      = c / len(irrelevan)\n",
    "    \n",
    "    for i in range(1,len(relevan)):\n",
    "        for j in range(len(relevan[i])):\n",
    "            rel[j] = rel[j] + relevan[i][j]\n",
    "            irrel[j] = irrel[j] + irrelevan[i][j]\n",
    "            \n",
    "    for i in range(len(rel)):\n",
    "        rel[i] = b*rel[i]\n",
    "        irrel[i] = c*irrel[i]\n",
    "        query[i] = a*query[i]\n",
    "        \n",
    "    for i in range(len(rel)):\n",
    "        exp.append(query[i]+rel[i]-irrel[i])\n",
    "        \n",
    "    for i in range(len(exp)):\n",
    "        if(exp[i]>0):\n",
    "            result[i] = exp[i]\n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GET NOT RELEVANCE DOC\n",
    "notRelevanceNumber = notRelevance(relevanceDocNumber, documentNumber)\n",
    "\n",
    "# GET DOCUMENT INDEX\n",
    "relevanceIndex    = getIndex(relevanceDocNumber,documentNumber)\n",
    "notRelevanceIndex = getIndex(notRelevanceNumber,documentNumber)\n",
    "\n",
    "# CONVERT TO VECTOR\n",
    "queryVec      = vector(query, terms)\n",
    "relevanVec    = []\n",
    "notRelevanVec = []\n",
    "\n",
    "for i in relevanceIndex:\n",
    "    relevanVec.append(vector(text[i], terms))\n",
    "    \n",
    "for i in notRelevanceIndex:\n",
    "    notRelevanVec.append(vector(text[i], terms))\n",
    "    \n",
    "# print(notRelevanVec)\n",
    "\n",
    "# QUERY EXPANSION VECTOR\n",
    "expansionVec = expansion(queryVec, relevanVec, notRelevanVec, 0.1, 0.1, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY EXPANSION VECTOR\n",
    "# expansionVec = expansion(queryVec, relevanVec, notRelevanVec, 1, 1, 0.5)\n",
    "# expansionVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "newQuery = []\n",
    "for i in expansionVec:\n",
    "    newQuery.append(terms[int(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'accept',\n",
       " 'account',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'alreadi',\n",
       " 'also',\n",
       " 'although',\n",
       " 'anxiou',\n",
       " 'area',\n",
       " 'ash',\n",
       " 'ask',\n",
       " 'assault',\n",
       " 'audit',\n",
       " 'avert',\n",
       " 'away',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'bankruptci',\n",
       " 'banqu',\n",
       " 'battl',\n",
       " 'beat',\n",
       " 'bello',\n",
       " 'bfce',\n",
       " 'bid',\n",
       " 'bnp',\n",
       " 'board',\n",
       " 'break',\n",
       " 'broader',\n",
       " 'burden',\n",
       " 'card',\n",
       " 'cash',\n",
       " 'ccf',\n",
       " 'cent',\n",
       " 'chairman',\n",
       " 'chargeur',\n",
       " 'chief',\n",
       " 'christian',\n",
       " 'ciri',\n",
       " 'claim',\n",
       " 'cloth',\n",
       " 'collaps',\n",
       " 'commerc',\n",
       " 'commerci',\n",
       " 'committe',\n",
       " 'compani',\n",
       " 'concess',\n",
       " 'control',\n",
       " 'convert',\n",
       " 'credit',\n",
       " 'creditor',\n",
       " 'de',\n",
       " 'deal',\n",
       " 'debt',\n",
       " 'derveloy',\n",
       " 'dewavrin',\n",
       " 'divis',\n",
       " 'dollar',\n",
       " 'du',\n",
       " 'due',\n",
       " 'earlier',\n",
       " 'emerg',\n",
       " 'employ',\n",
       " 'end',\n",
       " 'estim',\n",
       " 'even',\n",
       " 'execut',\n",
       " 'expect',\n",
       " 'expos',\n",
       " 'exterieur',\n",
       " 'facilit',\n",
       " 'famili',\n",
       " 'fear',\n",
       " 'fil',\n",
       " 'finalis',\n",
       " 'financ',\n",
       " 'financi',\n",
       " 'flatli',\n",
       " 'flipo',\n",
       " 'follow',\n",
       " 'formerli',\n",
       " 'franc',\n",
       " 'francais',\n",
       " 'french',\n",
       " 'gave',\n",
       " 'gener',\n",
       " 'general',\n",
       " 'give',\n",
       " 'got',\n",
       " 'govern',\n",
       " 'gravograph',\n",
       " 'group',\n",
       " 'hammer',\n",
       " 'head',\n",
       " 'heavili',\n",
       " 'high',\n",
       " 'hire',\n",
       " 'howev',\n",
       " 'immedi',\n",
       " 'includ',\n",
       " 'industri',\n",
       " 'interministeri',\n",
       " 'intern',\n",
       " 'jalla',\n",
       " 'keep',\n",
       " 'lifeboat',\n",
       " 'loan',\n",
       " 'loss',\n",
       " 'lyonnai',\n",
       " 'manag',\n",
       " 'mani',\n",
       " 'market',\n",
       " 'medium',\n",
       " 'ministri',\n",
       " 'much',\n",
       " 'national',\n",
       " 'necessari',\n",
       " 'need',\n",
       " 'negoti',\n",
       " 'news',\n",
       " 'nord',\n",
       " 'northern',\n",
       " 'number',\n",
       " 'offer',\n",
       " 'one',\n",
       " 'oper',\n",
       " 'outright',\n",
       " 'own',\n",
       " 'pari',\n",
       " 'particip',\n",
       " 'peac',\n",
       " 'pennel',\n",
       " 'peopl',\n",
       " 'per',\n",
       " 'pledg',\n",
       " 'portion',\n",
       " 'profit',\n",
       " 'propos',\n",
       " 'prouvost',\n",
       " 'prove',\n",
       " 'provid',\n",
       " 'reconstruct',\n",
       " 'reduc',\n",
       " 'refus',\n",
       " 'regi',\n",
       " 'replac',\n",
       " 'rescu',\n",
       " 'resign',\n",
       " 'result',\n",
       " 'ridden',\n",
       " 'save',\n",
       " 'secur',\n",
       " 'see',\n",
       " 'sell',\n",
       " 'sever',\n",
       " 'share',\n",
       " 'short',\n",
       " 'show',\n",
       " 'sign',\n",
       " 'societ',\n",
       " 'sold',\n",
       " 'stake',\n",
       " 'stock',\n",
       " 'struggl',\n",
       " 'subsidiari',\n",
       " 'succeed',\n",
       " 'suffer',\n",
       " 'supervis',\n",
       " 'surviv',\n",
       " 'symbol',\n",
       " 'temporari',\n",
       " 'term',\n",
       " 'textil',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'togeth',\n",
       " 'total',\n",
       " 'trade',\n",
       " 'understood',\n",
       " 'undertaken',\n",
       " 'unemploy',\n",
       " 'unit',\n",
       " 'use',\n",
       " 'vehicl',\n",
       " 'vev',\n",
       " 'weekend',\n",
       " 'wool',\n",
       " 'work',\n",
       " 'wors',\n",
       " 'yet']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['abandon', 'accept', 'account', 'agre', 'agreement', 'alreadi', 'also', 'although', 'anxiou', 'area', 'ash', 'ask', 'assault', 'audit', 'avert', 'away', 'bank', 'banker', 'bankruptci', 'banqu', 'battl', 'beat', 'bello', 'bfce', 'bid', 'bnp', 'board', 'break', 'broader', 'burden', 'card', 'cash', 'ccf', 'cent', 'chairman', 'chargeur', 'chief', 'christian', 'ciri', 'claim', 'cloth', 'collaps', 'commerc', 'commerci', 'committe', 'compani', 'concess', 'control', 'convert', 'credit', 'creditor', 'de', 'deal', 'debt', 'derveloy', 'dewavrin', 'divis', 'dollar', 'du', 'due', 'earlier', 'emerg', 'employ', 'end', 'estim', 'even', 'execut', 'expect', 'expos', 'exterieur', 'facilit', 'famili', 'fear', 'fil', 'finalis', 'financ', 'financi', 'flatli', 'flipo', 'follow', 'formerli', 'franc', 'francais', 'french', 'gave', 'gener', 'general', 'give', 'got', 'govern', 'gravograph', 'group', 'hammer', 'head', 'heavili', 'high', 'hire', 'howev', 'immedi', 'includ', 'industri', 'interministeri', 'intern', 'jalla', 'keep', 'lifeboat', 'loan', 'loss', 'lyonnai', 'manag', 'mani', 'market', 'medium', 'ministri', 'much', 'national', 'necessari', 'need', 'negoti', 'news', 'nord', 'northern', 'number', 'offer', 'one', 'oper', 'outright', 'own', 'pari', 'particip', 'peac', 'pennel', 'peopl', 'per', 'pledg', 'portion', 'profit', 'propos', 'prouvost', 'prove', 'provid', 'reconstruct', 'reduc', 'refus', 'regi', 'replac', 'rescu', 'resign', 'result', 'ridden', 'save', 'secur', 'see', 'sell', 'sever', 'share', 'short', 'show', 'sign', 'societ', 'sold', 'stake', 'stock', 'struggl', 'subsidiari', 'succeed', 'suffer', 'supervis', 'surviv', 'symbol', 'temporari', 'term', 'textil', 'thought', 'time', 'togeth', 'total', 'trade', 'understood', 'undertaken', 'unemploy', 'unit', 'use', 'vehicl', 'vev', 'weekend', 'wool', 'work', 'wors', 'yet'] \n",
      "\n",
      "\n",
      "RESULTS: \n",
      "\n",
      "\n",
      "FT  14 MAY 91 / London Stock Exchange: FT-SE 2,500 lost in nervous trading\n",
      "Document Number:  12\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / International Company News: Temporary rescue for French\n",
      "group\n",
      "Document Number:  31\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / Leading Article: A compromise path to Emu\n",
      "Document Number:  48\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "FT  14 MAY 91 / UK News (Employment): Harrods to be asked to abandon pay\n",
      "freeze\n",
      "Document Number:  74\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['12', '31', '48', '74']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUERY\n",
    "\n",
    "# raw_query = [\"abandoned abbott company\"]\n",
    "raw_query = newQuery\n",
    "\n",
    "expQuery = removePunctuation(raw_query)\n",
    "expQuery = caseFolding(expQuery)\n",
    "expQuery = tokenize(expQuery)\n",
    "expQuery = stopwordRemove(expQuery)\n",
    "expQuery = numberRemove(expQuery)\n",
    "expQuery = stemming(expQuery)\n",
    "expQuery = expQuery[0]\n",
    "\n",
    "# Check Query In Index\n",
    "expQuery = queryInIndex(expQuery, index)\n",
    "\n",
    "# RANKED RETRIEVAL\n",
    "\n",
    "N               = documentTotal\n",
    "tfidf_list      = []\n",
    "\n",
    "docFrequency    = df(expQuery, index)\n",
    "invDocFrequency = idf(docFrequency, N)\n",
    "termFrequency   = tf(expQuery, index)\n",
    "TFIDF           = tfidf(termFrequency, invDocFrequency)\n",
    "sc              = score(TFIDF)\n",
    "\n",
    "relevanceDocNumber = []\n",
    "count = 0\n",
    "print('Query: ', raw_query,'\\n\\n')\n",
    "print('RESULTS: \\n')\n",
    "\n",
    "for i in range(len(sc)):\n",
    "    relevanceDocNumber.append(int(sc[i]))\n",
    "    a = documentNumber.index(sc[i])\n",
    "    print(documentHeadline[a]+ 'Document Number: ',sc[i])\n",
    "#     print('\\nContent:')\n",
    "#     print(documentText[a][0:400], '[read more]>>')\n",
    "    print('-------------------------------------------\\n')\n",
    "    count = count + 1\n",
    "    if(count>=5):\n",
    "        break\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
